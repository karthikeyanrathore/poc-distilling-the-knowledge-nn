# "Distillation the Knowledge in Neural Network"
# https://arxiv.org/pdf/1503.02531

* Teacher / Student model
  * teacher model: bigger model (ensembel of smaller models)
  * student/distilled model: smaller model (it is trained on soft targets provided by teacher model)
  * soft targets provide the probability of each class (i.e the correct class with higher probability)
  * easier to deploy smaller models on edges devices.


* Log (22.02.2025 02:02)
 # MNIST distillation  'knowledge transfer' 
 (60000, 28, 28)
 (60000,)
 (10000, 28, 28)

  
  Teacher Net accuracy:  98.22
  Teacher Net test_error: 178
  Epoch 1/3
  1875/1875 [==============================] - 1s 339us/step - loss: 1.4211 - accuracy: 0.7485
  Epoch 2/3
  1875/1875 [==============================] - 1s 336us/step - loss: 0.7435 - accuracy: 0.8544
  Epoch 3/3
  1875/1875 [==============================] - 1s 339us/step - loss: 0.5149 - accuracy: 0.8824
  313/313 [==============================] - 0s 238us/step
  Student Net accuracy, 0.8923
  Student Net test error, 1077.0000000000002

   # training student net with distilled knowledge transfer from teacher net
   Epoch 1/3
   1875/1875 [==============================] - 1s 335us/step - loss: 1.0187 - accuracy: 0.8056
   Epoch 2/3
   1875/1875 [==============================] - 1s 337us/step - loss: 0.4975 - accuracy: 0.8856
   Epoch 3/3
   1875/1875 [==============================] - 1s 336us/step - loss: 0.3805 - accuracy: 0.9033
   Distilled student Net accuracy, 0.906
   Distilled student Net error, 939.9999999999998
